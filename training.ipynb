{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"training.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNPimGDwn8XyQXOjsOb8LRA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"sh6KdaoQIhtR"},"source":["## **Elaborazione di Immagini Mediche**\n","### Contest 2021/22 - Segmentazione ghiandola prostatica in immagini MRI\n","\n","### Script di Training\n","\n","Rigazio Sofia, Roccaro Lucia, Romano Anastasio, Ruzzante Elena"]},{"cell_type":"markdown","metadata":{"id":"vBzvaDj3Io8B"},"source":["* Installazione delle librerie necessarie"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uZD1JvhVfQnI"},"outputs":[],"source":["# Before running the script, reset the runtime to factory reset (Runtime -> Factory Reset Runtime)\n","# and then change runtime type to GPU (Runtime -> Change runtime type)\n","\n","!pip install tensorflow==2.1.0\n","!pip install keras==2.3.1\n","!pip install segmentation_models==1.0.1\n","!pip install h5py==2.10.0 \n","!pip install plotly==5.3.1"]},{"cell_type":"markdown","metadata":{"id":"j0DGsHkaI3jT"},"source":["\n","\n","*   Collegamento a Google Drive e import delle librerie necessarie\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OpJAl9SpUB8D"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","import random\n","import numpy as np\n","import plotly.express as px\n","\n","from matplotlib import pyplot as plt\n","from tqdm import tqdm\n","from skimage.io import imread\n","from skimage.transform import resize\n","from skimage.segmentation import mark_boundaries\n","\n","from keras.callbacks import CSVLogger\n","from keras.callbacks import EarlyStopping\n","from keras.utils.np_utils import to_categorical\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.models import load_model\n","\n","from segmentation_models import Unet\n","\n","# aggiunti\n","from scipy import ndimage"]},{"cell_type":"markdown","metadata":{"id":"xe8q2nilVO0g"},"source":["*   Preparazione del dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v8qZACUQAsjm"},"outputs":[],"source":["current_dir = 'drive/MyDrive/Colab Notebooks/EIM/Contest 2021-22/'\n","dataset_name = 'DATASET_stu'\n","\n","# Path\n","TRAIN_IMG_path = os.path.join(current_dir,dataset_name,'train','images')\n","TRAIN_MASK_path = os.path.join(current_dir,dataset_name,'train','manual')\n","VAL_IMG_path = os.path.join(current_dir,dataset_name,'val','images')\n","VAL_MASK_path = os.path.join(current_dir,dataset_name,'val','manual')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C9vABYz1bM5l"},"outputs":[],"source":["# Caricamento dataset in formato .rar da Google Drive a Colab\n","!pip install unrar\n","!unrar x \"drive/MyDrive/Colab Notebooks/EIM/Contest 2021-22/DATASET_stu.rar\" \"drive/MyDrive/Colab Notebooks/EIM/Contest 2021-22/\""]},{"cell_type":"markdown","metadata":{"id":"CkBPd04ucaw3"},"source":["* Estrazione della lista di volumi del training e validation set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IV4-vMZeVYZe"},"outputs":[],"source":["train_images = os.listdir(TRAIN_IMG_path)\n","n_images_train = len(train_images)\n","val_images = os.listdir(VAL_IMG_path)\n","n_images_val = len(val_images)"]},{"cell_type":"markdown","metadata":{"id":"I6iKIY8EJN4G"},"source":["* Lettura di un'immagine e della rispettiva maschera e settaggio dei parametri"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WyOM6TBSo-66"},"outputs":[],"source":["# Lettura di un generico volume e rispettiva maschera manuale\n","chosen_image_name = train_images[0] # prendiamo la prima immagine\n","original_volume = imread(os.path.join(TRAIN_IMG_path, chosen_image_name))\n","mask_manual = imread(os.path.join(TRAIN_MASK_path, chosen_image_name))\n","\n","# immagini\n","print(original_volume.shape)\n","print(original_volume.dtype)\n","# le immagini hanno dimensione (24,512,512) -> sono composte da 24 slice in grayscale\n","# il formato delle immagini è uint8\n","\n","# definiamo quindi i parametri\n","n_slices = original_volume.shape[0]\n","IMG_WIDTH = original_volume.shape[2] # la larghezza sono le colonne\n","IMG_HEIGHT = original_volume.shape[1] # l'altezza sono le righe\n","NUM_CLASSES = 2 # vogliamo segmentare due classi: prostata e background\n","IMG_CHANNELS = 3 # Le immagini fornite sono grayscale, per poter utilizzare i pesi preallenati di ImageNet forzeremo le immagini a RGB\n","\n","# maschere\n","print(mask_manual.dtype)\n","print(mask_manual.max())\n","# il formato delle maschere è uint8\n","# le maschere hanno valori 0 e 1 -> 0: background, 1: prostata"]},{"cell_type":"markdown","metadata":{"id":"dRY-q0NeJo5x"},"source":["* Rappresentazione a video del volume di un'immagine e della maschera corrispondente\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WWSimU29s2nO"},"outputs":[],"source":["# Rappresentazione a video del volume prostatico\n","fig = px.imshow(original_volume, animation_frame=0, binary_string=True, labels=dict(animation_frame=\"slice\"))\n","fig.show()\n","# Rappresentazione a video della corrispondente maschera binaria\n","fig = px.imshow(mask_manual, animation_frame=0, binary_string=True, labels=dict(animation_frame=\"slice\"))\n","fig.show()"]},{"cell_type":"markdown","metadata":{"id":"XEZ_VdZPMrrX"},"source":["* Visualizzazione del contorno dell'oggetto su una slice dell'immagine originale"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"INICXVFjvpvU"},"outputs":[],"source":["# Estrazione della decima slice\n","slice_chosen = 10\n","slice_10 = original_volume[slice_chosen-1,:,:]\n","mask_10 = mask_manual[slice_chosen-1,:,:]\n","\n","# Plot a video dell'immagine originale e della corrispondente segmentazione manuale\n","fig = plt.figure(figsize=(25,25))\n","ax1 = fig.add_subplot(1,3,1)\n","ax1.imshow(slice_10,cmap=plt.cm.gray), ax1.set_title('original image')\n","\n","ax2 = fig.add_subplot(1,3,2)\n","ax2.imshow(mask_10,cmap=plt.cm.gray), ax2.set_title('manual segmentation')    \n","\n","ax3 = fig.add_subplot(1,3,3)\n","ax3.imshow(mark_boundaries(slice_10,mask_10,color=(1,0,0))), ax3.set_title('image + manual')  "]},{"cell_type":"markdown","metadata":{"id":"68-Y3xdwq5yr"},"source":["* Definizione di una funzione che effettua il preprocessing delle immagini"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r2haSUvLqz4U"},"outputs":[],"source":["def preprocess(image,mask):  # quando copiato nello script di testing togliere maschera in input e output dalla funzione preprocess!!\n","\n","  # RIMOZIONE DEL BORDO NERO (SE PRESENTE)\n","  # rimuoviamo le colonne completamente nere\n","  idx = np.argwhere(np.all(image == 0, axis=0))\n","  image = np.delete(image, idx, axis=1)\n","  mask = np.delete(mask, idx, axis=1)\n","  # rimuoviamo le righe completamente nere\n","  idx = np.argwhere(np.all(image == 0, axis=1))\n","  image = np.delete(image, idx, axis=0)\n","  mask = np.delete(mask, idx, axis=0)\n","  # riportiamo alla dimensione originale (solo se ne abbiamo modificato le dimensioni)\n","  if image.shape != (IMG_HEIGHT,IMG_WIDTH):\n","    image = resize(image, (IMG_HEIGHT,IMG_WIDTH), preserve_range=True).astype(np.uint8)\n","    mask = resize(mask, (IMG_HEIGHT,IMG_WIDTH), preserve_range=True).astype(np.float32)\n","\n","  # filtro gaussiano\n","  def kernel_gauss(size, sigma):\n","    v = np.linspace(-(size-1)/2,(size-1)/2,size)\n","    x, y = np.meshgrid(v,v)\n","    h = np.exp(-((x**2 + y**2)/(2.0*sigma**2)))\n","    h = h/h.sum()\n","    return h\n","\n","  image = ndimage.correlate(image,kernel_gauss(3,3))\n","\n","  return image, mask"]},{"cell_type":"markdown","metadata":{"id":"Da-qL-myM6MA"},"source":["* Creazione delle matrici delle immagini e delle maschere"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lXog-RHQQn3V"},"outputs":[],"source":["def create_matrix (list_images,n_images,IMG_path,MASK_path):\n","  # creazione delle matrici che conterranno le slice\n","  X = np.zeros((n_images*n_slices,IMG_WIDTH,IMG_HEIGHT,IMG_CHANNELS), dtype=np.uint8) # immagini\n","  Y = np.zeros((n_images*n_slices,IMG_WIDTH,IMG_HEIGHT,NUM_CLASSES), dtype=np.float32) # maschere\n","\n","  for n, id_ in tqdm(enumerate(list_images), total=len(list_images)):\n","    # la variabile \"n\" rappresenta un contatore (0-num_immagini) mentre \"id_\" \n","    # contiene il nome della n-esima immagine\n","\n","    # Lettura dell'immagine e della maschera -> 24 slice di dimensione 512x512\n","    image = imread(os.path.join(IMG_path, id_))\n","    mask = imread(os.path.join(MASK_path, id_))\n","    \n","    for i in range(n_slices): # consideriamo una slice alla volta\n","      img_slice = image[i]\n","      mask_slice = mask[i]\n","      \n","      # Preprocessing\n","      [img_slice,mask_slice] = preprocess(img_slice,mask_slice)\n","\n","      # Forziamo l'immagine a RGB impilando 3 layer contenenti gli stessi valori \n","      X[n*n_slices+i] = np.stack((img_slice,img_slice,img_slice), axis = 2)\n","\n","      # Conversione della maschera in dato categorico\n","      mask_cat = to_categorical(mask_slice, num_classes=NUM_CLASSES, dtype='float32')\n","      # Inserimento della maschera nella matrice\n","      Y[n*n_slices+i] = mask_cat\n","\n","  return(X,Y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I3FO8FSVRbmj"},"outputs":[],"source":["# Training Set\n","print('Reading images - Training set')\n","[X_train,Y_train] = create_matrix(train_images,n_images_train,TRAIN_IMG_path,TRAIN_MASK_path)\n","\n","# Validation Set\n","print('\\nReading images - Validation set')\n","[X_val,Y_val] = create_matrix(val_images,n_images_val,VAL_IMG_path,VAL_MASK_path)\n"]},{"cell_type":"markdown","metadata":{"id":"rggjEcdYZxRu"},"source":["* Verifica di corretta creazione delle matrici"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f5Blw2XXXeWB"},"outputs":[],"source":["# Training Set\n","index = random.randint(0,n_images_train-1)\n","# Rappresentazione a video del volume prostatico\n","fig = px.imshow(X_train[index*n_slices:(index+1)*n_slices-1], animation_frame=0, labels=dict(animation_frame=\"slice\"))\n","fig.show()\n","# Rappresentazione a video della corrispondente maschera binaria\n","fig = px.imshow(Y_train[index*n_slices:(index+1)*n_slices-1,:,:,1], animation_frame=0, binary_string=True, labels=dict(animation_frame=\"slice\"))\n","fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LKECoVQYaUA8"},"outputs":[],"source":["# Validation Set\n","index = random.randint(0,n_images_val-1)\n","# Rappresentazione a video del volume prostatico\n","fig = px.imshow(X_val[index*n_slices:(index+1)*n_slices-1], animation_frame=0, binary_string=True, labels=dict(animation_frame=\"slice\"))\n","fig.show()\n","# Rappresentazione a video della corrispondente maschera binaria\n","fig = px.imshow(Y_val[index*n_slices:(index+1)*n_slices-1,:,:,1], animation_frame=0, binary_string=True, labels=dict(animation_frame=\"slice\"))\n","fig.show()"]},{"cell_type":"markdown","metadata":{"id":"t2Z_kPGePmvk"},"source":["* Definizione della funzione di Data Augmentation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oWBOOPBaacTo"},"outputs":[],"source":["# Data augmentation (training set)\n","image_datagen = ImageDataGenerator(rotation_range = 180,\n","\t\t\t\t\t\t\t\t\twidth_shift_range = 0.2,\n","\t\t\t\t\t\t\t\t\theight_shift_range = 0.2,\n","\t\t\t\t\t\t\t\t\thorizontal_flip = True,\n","\t\t\t\t\t\t\t\t\tvertical_flip = True,\n","\t\t\t\t\t\t\t\t\tfill_mode = 'reflect')\n","\n","# Data augmentation (validation set) -> non settiamo parametri perché fare data augmentation sul validation set è un errore\n","val_datagen = ImageDataGenerator()\n","\n","# Generator\n","seed = 1\n","def XYaugmentGenerator(X1, y, seed, batch_size):\n","\tgenX1 = image_datagen.flow(X1, y, batch_size=batch_size, seed=seed)\n","\tgenX2 = image_datagen.flow(y, X1, batch_size=batch_size, seed=seed)\n","\twhile True:\n","\t\tX1i = genX1.next()\n","\t\tX2i = genX2.next()\n","\t\tyield X1i[0], X2i[0]"]},{"cell_type":"markdown","metadata":{"id":"rD_gLRJwPvHg"},"source":["* Definizione del modello UNET"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YeAlNJcTEQQx"},"outputs":[],"source":["BACKBONE = 'resnet34'\n","model = Unet(backbone_name = BACKBONE,\n","            input_shape = (IMG_WIDTH,IMG_HEIGHT,IMG_CHANNELS),\n","            encoder_weights = 'imagenet', \n","            encoder_freeze = True,\n","            decoder_block_type = 'transpose',\n","            classes = NUM_CLASSES,\n","            activation = 'sigmoid')\n","\n","# Definizione algoritmo di ottimizzazione e funzione di loss\n","model.compile('Adam', loss='binary_crossentropy', metrics=['binary_accuracy'])"]},{"cell_type":"markdown","metadata":{"id":"C55IqTzWPxmn"},"source":["* Allenamento della rete"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yh3xWvL-aiQW"},"outputs":[],"source":["# Allenamento della rete\n","\n","# Parametri della rete\n","n_train_samples = n_images_train*n_slices # numero delle immagini di train\n","n_val_samples = n_images_val*n_slices # numero delle immagini di validation\n","batch_size = 8\n","n_epochs = 20\n","\n","# Checkpoint definition\n","csv_logger = CSVLogger('./log.out', append=True, separator=';')\n","earlystopping = EarlyStopping(monitor = 'val_binary_accuracy',verbose = 1, min_delta = 0.01, patience = 4, mode = 'max')\n","callbacks_list = [csv_logger, earlystopping]\n","\n","# Train model\n","results = model.fit_generator(XYaugmentGenerator(X_train,Y_train,seed, batch_size), \n","                              steps_per_epoch = np.ceil(float(n_train_samples)/float(batch_size)),\n","                              validation_data = val_datagen.flow(X_val,Y_val,batch_size), \n","                              validation_steps = np.ceil(float(n_val_samples)/float(batch_size)),\n","                              shuffle = True,\n","                              epochs = n_epochs,\n","                              callbacks = callbacks_list)"]},{"cell_type":"markdown","metadata":{"id":"rJsyzGtqP11t"},"source":["* Salvataggio del modello allenato"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w-ZQrw3val4X"},"outputs":[],"source":["model_path = current_dir + '/trained_net.h5'\n","model.save(model_path)"]}]}